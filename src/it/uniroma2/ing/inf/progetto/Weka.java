package it.uniroma2.ing.inf.progetto;
import weka.core.Instance;
import weka.core.Instances;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.InvalidPathException;
import java.text.DecimalFormat;
import java.util.ArrayList;

import weka.attributeSelection.CfsSubsetEval;
import weka.attributeSelection.GreedyStepwise;
import weka.classifiers.Classifier;
import weka.classifiers.misc.HyperPipes;
import weka.classifiers.misc.VFI;
import weka.classifiers.lazy.IB1;
import weka.classifiers.lazy.IBk;
import weka.filters.Filter;
import weka.filters.supervised.attribute.AttributeSelection;
import weka.filters.supervised.instance.SMOTE;
import weka.filters.unsupervised.attribute.NumericTransform;
import weka.filters.unsupervised.attribute.Remove;
import weka.filters.unsupervised.attribute.ReplaceMissingWithUserConstant;
import weka.filters.unsupervised.attribute.SwapValues;
import weka.core.converters.ArffSaver;
import weka.core.converters.CSVLoader;
import weka.core.converters.ConverterUtils.DataSource;
import weka.classifiers.functions.Logistic;
import weka.classifiers.functions.VotedPerceptron;
import weka.classifiers.trees.J48;
import weka.classifiers.trees.RandomForest;
import weka.classifiers.bayes.NaiveBayes;


public class Weka {


	private static final String ARFF=".arff";
	private static final String dirRQ2= "Results RQ2";

	int percentInstOfMajorityClass=0;
	String projectName;
	Instances noFilterTraining;
	Instances testing;	
	DecimalFormat numberFormat = new DecimalFormat("0.00");
	String myClassificator;



	//questo metodo compara i risultati dei tre classificatori utilizzando la tecnica WalkForward
	public void doPrediction(String type,String projectName, ArrayList<String> idList,ArrayList<String> sizeList) {
		//System.out.println(type+" "+projectName); // Class DIRSERVER

		String name = projectName+"_"+type; 
		// ------------------------- 

		try {


			String arffNameFileTrain = "";
			String arffNameFileTest = "";

			//prima ci si crea un file arff da quello csv

			// load CSV
			CSVLoader loader = new CSVLoader();
			loader.setFieldSeparator(";");
			loader.setSource(new File(projectName+"_"+type+"_Train.csv"));
			Instances data = loader.getDataSet();


			//  re-order of Nominal Values in ClassIndex 
			if (data.attribute(data.numAttributes()-1).value(0).equals("YES")){
				SwapValues swp = new SwapValues();
				swp.setAttributeIndex("last");
				swp.setInputFormat(data);
				data = Filter.useFilter(data, swp);
			}

			// re-order of Nominal Values for Fix commit metric 
			if (type.equals("Commit")&& data.attribute(9).value(0).equals("YES")){
				SwapValues swp = new SwapValues();
				swp.setAttributeIndex("9");
				swp.setInputFormat(data);
				data = Filter.useFilter(data, swp);
			}


			//Normalization 
			NumericTransform normFilter = new NumericTransform();
			normFilter.setMethodName("log10");
			normFilter.setAttributeIndices("first-last");
			normFilter.setInputFormat(data);
			data = Filter.useFilter(data, normFilter);

			//replace missing values generated by the normalization
			ReplaceMissingWithUserConstant repl= new ReplaceMissingWithUserConstant();
			repl.setNumericReplacementValue("0");
			repl.setNominalStringReplacementValue("NO");
			repl.setInputFormat(data);
			data =Filter.useFilter(data,repl);

			// save ARFF
			ArffSaver saver = new ArffSaver();
			saver.setInstances(data);

			arffNameFileTrain = projectName+"_"+type+"_Train"+ARFF;


			saver.setFile(new File(arffNameFileTrain));
			saver.writeBatch();


			//adesso ci si crea l'arff per il test
			// load CSV
			loader = new CSVLoader();
			loader.setFieldSeparator(";");
			loader.setSource(new File(projectName+"_"+type+"_Test.csv"));
			data = loader.getDataSet();

			//  re-order of Nominal Values in ClassIndex
			if (data.attribute(data.numAttributes()-1).value(0).equals("YES")){
				SwapValues swp = new SwapValues();
				swp.setAttributeIndex("last");
				swp.setInputFormat(data);
				data = Filter.useFilter(data, swp);
			}

			// re-order of Nominal Values for Fix commit metric 
			if (type.equals("Commit")&& data.attribute(9).value(0).equals("YES")){
				SwapValues swp = new SwapValues();
				swp.setAttributeIndex("9");
				swp.setInputFormat(data);
				data = Filter.useFilter(data, swp);
			}

			//Normalization 
			normFilter = new NumericTransform();
			normFilter.setMethodName("log10");
			normFilter.setAttributeIndices("first-last");
			normFilter.setInputFormat(data);
			data = Filter.useFilter(data, normFilter);

			//replace missing values generated by the normalization
			repl= new ReplaceMissingWithUserConstant();
			repl.setNumericReplacementValue("0");
			repl.setNominalStringReplacementValue("NO");
			repl.setInputFormat(data);
			data =Filter.useFilter(data,repl);

			// save ARFF
			saver = new ArffSaver();
			saver.setInstances(data);


			arffNameFileTest = projectName+"_"+type+"_Test"+ARFF;


			saver.setFile(new File(arffNameFileTest));
			saver.writeBatch();

			//load train dataset
			DataSource source1 = new DataSource(arffNameFileTrain);
			Instances training = source1.getDataSet();

			training.attribute(arffNameFileTest);
			//---------------------------
			//Feature Selection 
			//create AttributeSelection object
			AttributeSelection filter = new AttributeSelection();
			//create evaluator and search algorithm objects
			CfsSubsetEval subEval = new CfsSubsetEval();
			GreedyStepwise search = new GreedyStepwise();
			//set the algorithm to search backward
			search.setSearchBackwards(true);
			//set the filter to use the evaluator and search algorithm
			filter.setEvaluator(subEval);
			filter.setSearch(search);



			//specify the dataset
			filter.setInputFormat(training);

			//qui si crea il training filtrato
			training = Filter.useFilter(training, filter);

			//---------------------------------	


			//load test dataset
			DataSource source2 = new DataSource(arffNameFileTest);
			Instances myTest = source2.getDataSet();

			//stima numero attributi con i filtri
			int numAttr = training.numAttributes();
			training.setClassIndex(numAttr - 1); //leviamo 1 perchè l'ultima colonna la vogliamo stimare 


			myTest= Filter.useFilter(myTest, filter);
			myTest.setClassIndex(numAttr - 1);


			int numtesting = myTest.numInstances();

			// make sure they're compatible
			String msg = training.equalHeadersMsg(myTest);
			if (msg != null)
				throw new Exception(msg);


			//-----------------
			//
			// count of instances with distinct buggy value
			int[] countAttributes = new int[training.get(numAttr-1).numValues()];
			for(Instance instance: training){
				countAttributes[(int)instance.value(numAttr-1)]++;
			}
			//System.out.println(countAttributes[0]);
			//System.out.println(countAttributes[1]+" **");

			//SMOTE
			SMOTE smote = new SMOTE();
			smote.setInputFormat(training);
			if ((double)((double)(countAttributes[0]-(countAttributes[1]))/(double)(countAttributes[1]))*100>0.0) {
				smote.setPercentage((double)((double)(countAttributes[0]-(countAttributes[1]))/(double)(countAttributes[1]))*100);
			}
			else {
				smote.setPercentage((double)((double)(countAttributes[1]-(countAttributes[0]))/(double)(countAttributes[0]))*100);
			}
			smote.setClassValue("0");
			training = Filter.useFilter(training, smote); //Apply SMOTE on Dataset

			//System.out.println((double)((double)(countAttributes[0]-(countAttributes[1]))/(double)(countAttributes[1]))*100);
			//-----------------			

			Classifier classifier = null;
			FileWriter fileWriter = null;

			int numOfClassifiers = 9;
			//per ogni classificatore
			for(int n=1;n<numOfClassifiers+1;n++) {

				//for(int n=1;n<5;n++) {
				if(n==1) {
					//RandomForest---------------
					classifier = new RandomForest(); //scelgo come classificatore RandomForest
					myClassificator ="RF";
					classifier.buildClassifier(training); //qui si fa il training

					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");

				}

				else if (n==2) {
					//J48---------------
					classifier = new J48(); //scelgo come classificatore random forest
					myClassificator ="J48";
					classifier.buildClassifier(training); //qui si fa il training

					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");

				}
				else if (n==3) {

					//Hyper Pipes---------------
					classifier = new HyperPipes(); 
					myClassificator ="HP";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}

				else if (n==4) {
					//IB1---------------
					classifier = new IB1(); 
					myClassificator ="IB1";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}
				else if (n==5) {
					//IBk---------------
					classifier = new IBk(); 
					myClassificator ="IBk";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}
				else if (n==6) {
					//VotedPerceptron ---------------
					classifier = new VotedPerceptron(); 
					myClassificator ="VP";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}

				else if (n==7) {
					//NAIVE BAYES ---------------
					classifier = new NaiveBayes(); 
					myClassificator ="NB";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}

				else if (n==8) {
					//LOGISTIC---------------
					classifier = new Logistic(); 
					myClassificator ="LOG";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}
				if (n==9) {
					//VFI---------------
					classifier = new VFI(); 
					myClassificator ="VFI";
					classifier.buildClassifier(training); //qui si fa il training
					//ora si scrive l'header del file csv coi risultati
					fileWriter = new FileWriter(dirRQ2+"\\"+name+"_"+myClassificator+".csv");
				}



				//---------------------------------------//
				fileWriter.append("ID;Size;Predicted;Actual");
				fileWriter.append("\n");


				// Loop over each test instance.
				for (int i = 0; i < numtesting; i++)
				{
					// Get the true class label from the instance's own classIndex.
					String trueClassLabel = 
							myTest.instance(i).toString(myTest.classIndex());

					// Get the prediction probability distribution.
					double[] predictionDistribution = 
							classifier.distributionForInstance(myTest.instance(i)); 

					// Get the probability.
					double predictionProbability = 
							predictionDistribution[1]; //1==prob YES

					fileWriter.append(idList.get(i));
					fileWriter.append(";");
					fileWriter.append(sizeList.get(i));
					fileWriter.append(";");
					fileWriter.append(String.format("%6.3f",predictionProbability).replace(',', '.'));
					fileWriter.append(";");
					fileWriter.append(trueClassLabel);
					fileWriter.append("\n");


				}

				fileWriter.close();

			}

			saver.resetWriter();
			myTest.clear();
			data.clear();

			//recursiveDelete(new File(projectName+"_"+type+"_Train"+ARFF));
			//recursiveDelete(new File(projectName+"_"+type+"_Test"+ARFF));
			//recursiveDelete(new File(projectName+"_"+type+"_Train.csv"));			
			//recursiveDelete(new File(projectName+"_"+type+"_Test.csv"));



		}
		catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		} 
	}

	public ArrayList<String> doFeatureSelectionForMethod(String filePath) {
		ArrayList<String> features = null;
		try {
			CSVLoader loader = new CSVLoader();
			loader.setSource(new File(filePath));
			loader.setFieldSeparator(";");
			Instances data = loader.getDataSet();


			//remove of ID and Size feature
			Remove removeFilter = new Remove();
			int[] indices = {0, 1};
			removeFilter.setAttributeIndicesArray(indices);
			removeFilter.setInputFormat(data);
			data = Filter.useFilter(data, removeFilter);

			data.setClassIndex(data.numAttributes() - 1); //leviamo 1 perchè l'ultima colonna la vogliamo stimare 

			//Feature Selection 
			//create AttributeSelection object
			AttributeSelection filter = new AttributeSelection();
			//create evaluator and search algorithm objects
			CfsSubsetEval subEval = new CfsSubsetEval();
			GreedyStepwise search = new GreedyStepwise();
			//set the filter to use the evaluator and search algorithm
			filter.setEvaluator(subEval);
			filter.setSearch(search);

			//specify the dataset
			filter.setInputFormat(data);

			//qui si crea il training filtrato
			data = Filter.useFilter(data, filter);

			features = new ArrayList<>();
			//get feature names
			for (int i=0; i<data.numAttributes();i++) {
				features.add(data.attribute(i).name());
				//System.out.println(data.attribute(i).name());
			}


		} catch (Exception e) {

			e.printStackTrace();
			System.exit(-1);
		}
		return features;

	}

	public ArrayList<String> doFeatureSelectionForClass(String filePath) {
		ArrayList<String> features = null;
		try {
			CSVLoader loader = new CSVLoader();
			loader.setSource(new File(filePath));
			loader.setFieldSeparator(";");
			Instances data = loader.getDataSet();


			//remove of ID and Size feature
			Remove removeFilter = new Remove();
			int[] indices = {0, 1};
			removeFilter.setAttributeIndicesArray(indices);
			removeFilter.setInputFormat(data);
			data = Filter.useFilter(data, removeFilter);

			data.setClassIndex(data.numAttributes() - 1); //leviamo 1 perchè l'ultima colonna la vogliamo stimare 

			//Feature Selection 
			//create AttributeSelection object
			AttributeSelection filter = new AttributeSelection();
			//create evaluator and search algorithm objects
			CfsSubsetEval subEval = new CfsSubsetEval();
			GreedyStepwise search = new GreedyStepwise();
			//set the filter to use the evaluator and search algorithm
			filter.setEvaluator(subEval);
			filter.setSearch(search);

			//specify the dataset
			filter.setInputFormat(data);

			//qui si crea il training filtrato
			data = Filter.useFilter(data, filter);

			features = new ArrayList<>();
			//get feature names
			for (int i=0; i<data.numAttributes();i++) {
				features.add(data.attribute(i).name());
				//System.out.println(data.attribute(i).name());
			}


		} catch (Exception e) {

			e.printStackTrace();
			System.exit(-1);
		}
		return features;

	}

	public static void recursiveDelete(File file) {
		//to end the recursive loop
		if (!file.exists())
			return;

		//if directory exists, go inside and call recursively
		if (file.isDirectory()) {
			for (File f : file.listFiles()) {
				//call recursively
				recursiveDelete(f);
			}
		}
		//call delete to delete files and empty directory
		try {
			//disabling Read Only property of the file to be deleted resolves the issue triggered by Files.delete
			Files.setAttribute(file.toPath(), "dos:readonly", false);
			Files.deleteIfExists(file.toPath());
		} catch (IOException| InvalidPathException e) {
			e.printStackTrace();
			System.exit(-1);
		}
	}


}
